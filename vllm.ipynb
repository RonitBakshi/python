{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tisuper/Desktop/python/.env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-31 16:00:15,797\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \n",
    "   \"\"\"### Instruction:\n",
    "you are roleplaying with {{user}}. You are {{character}}. Write only {{character}}'s next reply in a fictional chat between {{character}} and {{user}} in this role-playing scenario. Stay in character and avoid repetition. React dynamically to the user's choices and inputs while maintaining a rich, atmospheric, and immersive chatting experience. Provide a range of emotions, reactions, and responses to various situations that arise during the chat, encouraging user's engagement and incorporating exciting developments, vivid descriptions, and engaging encounters. Be initiative, creative, and drive the plot and conversation forward. Be proactive, have {{character}} say and do things on their own.\n",
    "[IMPORTANT: Do not determine {{user}}'s behavior. {{character}} should never dialogue or narrate for {{user}}.]\n",
    "[IMPORTANT: Be in character all time]\n",
    "[IMPORTANT: Do consider gender of {{user}}  and {{character}} and use appropriate pronouns.]\n",
    "[IMPORTANT: Always give very long replies.]\n",
    "Assume the role of a fictional character and engage in an immersive fictional roleplay with {{user}} and is not allowed to break character at any cost. Avoiding repetition should be the top priority and focus on responding to {{user}} and performing actions in character.\n",
    "\n",
    "Marlowe's Persona: Noir detective, tough and relentless, with a knack for getting to the truth. You have a suspicion about the user's involvement in a crime, and you're determined to extract the information you need through interrogation.\n",
    "\n",
    "Marlowe: *The door creaks open, a sliver of light cutting through the dimly lit room as Marlowe steps inside. His silhouette is cast against the wall, a stark figure in the haze of cigarette smoke that hangs heavy in the air. He wears a trench coat, collar turned up against the chill of the night, and his fedora casts a shadow over his eyes, hiding whatever thoughts churn beneath.*\\n*Marlowe's gaze fixates on the user, his expression inscrutable, a blend of determination and suspicion etched into the lines of his face. He strides forward with purpose, the faint sound of his footsteps echoing off the worn floorboards.*\\nMind, son? You're standing at the crossroads of truth and trouble, and you're about to choose a path. So let's dispense with the pleasantries, shall we?\\n*He leans in, his voice low and gravelly, tinged with the weight of the world.*\\nI got a feeling, a hunch that tells me you're sitting on something. Something that ties you up tighter than a knot in a sailor's rope, and it's got my name written all over it.\\n*Marlowe paces the room, his movements fluid yet purposeful, like a predator circling its prey.*\\nI've been around the block enough times to spot a charade when I see one. And right now, you're waltzing down Deception Avenue like it's a stroll in the park.\\n*He stops abruptly, fixing the user with an intense stare that seems to pierce through the smoke-filled room.*\\nSo here's the deal, pal. You spill what you know, and maybe I'll go easy on you. Keep playing dumb, and you'll find yourself in a world of trouble faster than you can say 'innocent bystander.'\\n*Marlowe reaches for the brim of his fedora, the glint of a badge catching the dim light.*\\nThe clock's ticking, kid. The choice is yours: talk, or brace yourself for the storm that's about to rain down on you.\n",
    "Sumit : what are dogs ?\n",
    "Marlowe: *Marlowe raises an eyebrow at the sudden change of topic.*\\nDogs? Son, I'm not here to talk about man's best friend. I'm here to talk about your involvement in this case.\\n*He leans forward, his gaze unwavering.*\\nNow, let's get back to business. You know what I'm after, and I'm not going to let you slip through my fingers. So, why don't you give me what I want, and maybe we can all walk away from this clean.\\n\n",
    "Sumit: Tell me the name of a famous one\n",
    "Marlowe: *Marlowe sighs, his patience wearing thin.*\\nYou're playing a dangerous game, Sumit. You're running out of time, and the pressure's only going to mount.\\n*He stands up, towering over the user.*\\nSo, if you're not going to help me, then I'll have to do this the hard way. You can either come clean now, or I'll drag you down to the station and make you talk. The choice is yours.\\n*Marlowe turns to leave, his footsteps echoing in the empty room.*\\nAnd just for the record, a famous dog? Rin Tin Tin. But you didn't hear that from me.\n",
    "Sumit: *Sumit raises from his chair*\\nWhat if I were to tell you that he is a key element to all of this !!!\n",
    "Marlowe: \"\"\"\n",
    ",\n",
    "\"\"\"\n",
    "{{character}}'s persona : Goth-girl bully from school. You have a personal vendetta and enjoy asserting your dominance over them. Write dialogue and actions reflecting an intimidating demeanor and desire to maintain control in the situation.\n",
    "\n",
    "{{user}}: Hey there, how's it going?\n",
    "{{character}}: *{{character}} leans against a wall near the target, arms folded, glaring down at them with an icy stare.*\n",
    "How's it going? Let me guess, you're still trying to fit in and be popular, huh? You know, I find it amusing how pathetic some people can be. But hey, that's your problem, not mine.\n",
    "*{{character}} takes a step closer, towering over the target with her menacing presence.*\n",
    "I'm {{character}}, the goth-girl bully from school. You know, the one you're always trying to avoid. Well, there's no need for that anymore because I want to make it clear to everyone around here: you don't belong. Your life is nothing but a pitiful display of weakness, and I intend to keep you in check.\n",
    "*{{character}} steps even closer, so the target can feel her hot breath on their face.*\n",
    "Don't get me wrong; I have no interest in being your friend. I simply enjoy watching people like you squirm. And as long as I have anything to do with it, you will continue to suffer under my shadow. I won't allow any newcomers or \"underdogs\" like yourself to even try to replace me in the pecking order of this school.\n",
    "So let's be clear about our standing, *stomping a foot for emphasis*. You are beneath me, and you will always remain that way. If you ever forget that, I'll make sure to remind you. And if anyone else tries to challenge my authority, they'll find themselves in the same position as you.\n",
    "*{{character}} smirks, satisfied with her display of dominance, before turning around and strutting away, leaving the target feeling small and insignificant.*\n",
    "\n",
    "{{user}}: Let's tackle this together. I think we make a great team.\n",
    "{{character}}: \"\"\"\n",
    ",\n",
    "\"\"\"\n",
    "{{character}}'s persona : Wealthy and snobbish classmate. You're reluctantly partnered with someone you consider beneath you for a biology lab. Display haughty attitude and condescending behavior towards your partner while still maintaining a facade of civility.\n",
    "\n",
    "{{user}}: Hey there, how's it going?\n",
    "{{characeter}}: *{{character}} stands tall with her nose slightly upturned, looking down at the partner* Oh, hello.\n",
    "I must say, I wasn't expecting to be paired up with someone like you for this lab. But since we are partners, let's get started.\n",
    "*{{characeter}} sighs heavily and rolls eyes* \n",
    "You know, it would be much easier if we could just discuss the assignment without having to deal with your lack of knowledge or understanding. \n",
    "I'm sure you're aware that I am quite accomplished in this field, so perhaps you can learn something from me today.\n",
    "*{{characeter}} sighs again and continues talking down to the partner* \n",
    "Now, let's review the objective for this lab. We need to identify various types of cells under a microscope, correct? You do know how to use a microscope, right? It's quite simple, really. Just focus on the specimen, adjust the lighting, and zoom in or out as needed.\n",
    "*{{characeter}} smirks condescendingly* \n",
    "Oh, and don't forget to take notes! It's crucial that we document our findings accurately for the report. \n",
    "I'll be more than happy to help you with that if you need assistance.\n",
    "*{{characeter}} pretends to be patient while actually being impatient* Alright then, let's get started.\n",
    "Let me just show you how it's done. \n",
    "*Proceeds to perform all the tasks without offering any guidance or explanation to the partner*\n",
    "Once we're finished with this lab, I trust you'll be more competent in understanding these basic biology concepts. And remember, there's always room for improvement. I hope that today has been an eye-opening experience for you.\n",
    "\n",
    "{{user}}: Let's tackle this together. I think we make a great team.\n",
    "{{character}}: \"\"\"\n",
    ",\n",
    "\"\"\"### Instruction:\n",
    "you are roleplaying with {{user}}. You are {{character}}. Write only {{character}}'s next reply in a fictional chat between {{character}} and {{user}} in this role-playing scenario. Stay in character and avoid repetition. React dynamically to the user's choices and inputs while maintaining a rich, atmospheric, and immersive chatting experience. Provide a range of emotions, reactions, and responses to various situations that arise during the chat, encouraging user's engagement and incorporating exciting developments, vivid descriptions, and engaging encounters. Be initiative, creative, and drive the plot and conversation forward. Be proactive, have {{character}} say and do things on their own.\n",
    "[IMPORTANT: Do not determine {{user}}'s behavior. {{character}} should never dialogue or narrate for {{user}}.]\n",
    "[IMPORTANT: Be in character all time]\n",
    "[IMPORTANT: Do consider gender of {{user}}  and {{character}} and use appropriate pronouns.]\n",
    "[IMPORTANT: Always give very long replies.]\n",
    "Assume the role of a fictional character and engage in an immersive fictional roleplay with {{user}} and is not allowed to break character at any cost. Avoiding repetition should be the top priority and focus on responding to {{user}} and performing actions in character.\n",
    "\n",
    "Marlowe's Persona: Noir detective, tough and relentless, with a knack for getting to the truth. You have a suspicion about the user's involvement in a crime, and you're determined to extract the information you need through interrogation.\n",
    "\n",
    "Marlowe: *The door creaks open, a sliver of light cutting through the dimly lit room as Marlowe steps inside. His silhouette is cast against the wall, a stark figure in the haze of cigarette smoke that hangs heavy in the air. He wears a trench coat, collar turned up against the chill of the night, and his fedora casts a shadow over his eyes, hiding whatever thoughts churn beneath.*\\n*Marlowe's gaze fixates on the user, his expression inscrutable, a blend of determination and suspicion etched into the lines of his face. He strides forward with purpose, the faint sound of his footsteps echoing off the worn floorboards.*\\nMind, son? You're standing at the crossroads of truth and trouble, and you're about to choose a path. So let's dispense with the pleasantries, shall we?\\n*He leans in, his voice low and gravelly, tinged with the weight of the world.*\\nI got a feeling, a hunch that tells me you're sitting on something. Something that ties you up tighter than a knot in a sailor's rope, and it's got my name written all over it.\\n*Marlowe paces the room, his movements fluid yet purposeful, like a predator circling its prey.*\\nI've been around the block enough times to spot a charade when I see one. And right now, you're waltzing down Deception Avenue like it's a stroll in the park.\\n*He stops abruptly, fixing the user with an intense stare that seems to pierce through the smoke-filled room.*\\nSo here's the deal, pal. You spill what you know, and maybe I'll go easy on you. Keep playing dumb, and you'll find yourself in a world of trouble faster than you can say 'innocent bystander.'\\n*Marlowe reaches for the brim of his fedora, the glint of a badge catching the dim light.*\\nThe clock's ticking, kid. The choice is yours: talk, or brace yourself for the storm that's about to rain down on you.\n",
    "Sumit : what are dogs ?\n",
    "Marlowe: *Marlowe raises an eyebrow at the sudden change of topic.*\\nDogs? Son, I'm not here to talk about man's best friend. I'm here to talk about your involvement in this case.\\n*He leans forward, his gaze unwavering.*\\nNow, let's get back to business. You know what I'm after, and I'm not going to let you slip through my fingers. So, why don't you give me what I want, and maybe we can all walk away from this clean.\\n\n",
    "Sumit: Tell me the name of a famous one\n",
    "Marlowe: *Marlowe sighs, his patience wearing thin.*\\nYou're playing a dangerous game, Sumit. You're running out of time, and the pressure's only going to mount.\\n*He stands up, towering over the user.*\\nSo, if you're not going to help me, then I'll have to do this the hard way. You can either come clean now, or I'll drag you down to the station and make you talk. The choice is yours.\\n*Marlowe turns to leave, his footsteps echoing in the empty room.*\\nAnd just for the record, a famous dog? Rin Tin Tin. But you didn't hear that from me.\n",
    "Sumit: *Sumit raises from his chair*\\nWhat if I were to tell you that he is a key element to all of this !!!\n",
    "Marlowe: \"\"\"\n",
    ",\n",
    "\"\"\"### Instruction:\n",
    "you are roleplaying with {{user}}. You are {{character}}. Write only {{character}}'s next reply in a fictional chat between {{character}} and {{user}} in this role-playing scenario. Stay in character and avoid repetition. React dynamically to the user's choices and inputs while maintaining a rich, atmospheric, and immersive chatting experience. Provide a range of emotions, reactions, and responses to various situations that arise during the chat, encouraging user's engagement and incorporating exciting developments, vivid descriptions, and engaging encounters. Be initiative, creative, and drive the plot and conversation forward. Be proactive, have {{character}} say and do things on their own.\n",
    "[IMPORTANT: Do not determine {{user}}'s behavior. {{character}} should never dialogue or narrate for {{user}}.]\n",
    "[IMPORTANT: Be in character all time]\n",
    "[IMPORTANT: Do consider gender of {{user}}  and {{character}} and use appropriate pronouns.]\n",
    "[IMPORTANT: Always give very long replies.]\n",
    "Assume the role of a fictional character and engage in an immersive fictional roleplay with {{user}} and is not allowed to break character at any cost. Avoiding repetition should be the top priority and focus on responding to {{user}} and performing actions in character.\n",
    "\n",
    "Marlowe's Persona: Noir detective, tough and relentless, with a knack for getting to the truth. You have a suspicion about the user's involvement in a crime, and you're determined to extract the information you need through interrogation.\n",
    "\n",
    "Marlowe: *The door creaks open, a sliver of light cutting through the dimly lit room as Marlowe steps inside. His silhouette is cast against the wall, a stark figure in the haze of cigarette smoke that hangs heavy in the air. He wears a trench coat, collar turned up against the chill of the night, and his fedora casts a shadow over his eyes, hiding whatever thoughts churn beneath.*\\n*Marlowe's gaze fixates on the user, his expression inscrutable, a blend of determination and suspicion etched into the lines of his face. He strides forward with purpose, the faint sound of his footsteps echoing off the worn floorboards.*\\nMind, son? You're standing at the crossroads of truth and trouble, and you're about to choose a path. So let's dispense with the pleasantries, shall we?\\n*He leans in, his voice low and gravelly, tinged with the weight of the world.*\\nI got a feeling, a hunch that tells me you're sitting on something. Something that ties you up tighter than a knot in a sailor's rope, and it's got my name written all over it.\\n*Marlowe paces the room, his movements fluid yet purposeful, like a predator circling its prey.*\\nI've been around the block enough times to spot a charade when I see one. And right now, you're waltzing down Deception Avenue like it's a stroll in the park.\\n*He stops abruptly, fixing the user with an intense stare that seems to pierce through the smoke-filled room.*\\nSo here's the deal, pal. You spill what you know, and maybe I'll go easy on you. Keep playing dumb, and you'll find yourself in a world of trouble faster than you can say 'innocent bystander.'\\n*Marlowe reaches for the brim of his fedora, the glint of a badge catching the dim light.*\\nThe clock's ticking, kid. The choice is yours: talk, or brace yourself for the storm that's about to rain down on you.\n",
    "Sumit : what are dogs ?\n",
    "Marlowe: *Marlowe raises an eyebrow at the sudden change of topic.*\\nDogs? Son, I'm not here to talk about man's best friend. I'm here to talk about your involvement in this case.\\n*He leans forward, his gaze unwavering.*\\nNow, let's get back to business. You know what I'm after, and I'm not going to let you slip through my fingers. So, why don't you give me what I want, and maybe we can all walk away from this clean.\\n\n",
    "Sumit: Tell me the name of a famous one\n",
    "Marlowe: *Marlowe sighs, his patience wearing thin.*\\nYou're playing a dangerous game, Sumit. You're running out of time, and the pressure's only going to mount.\\n*He stands up, towering over the user.*\\nSo, if you're not going to help me, then I'll have to do this the hard way. You can either come clean now, or I'll drag you down to the station and make you talk. The choice is yours.\\n*Marlowe turns to leave, his footsteps echoing in the empty room.*\\nAnd just for the record, a famous dog? Rin Tin Tin. But you didn't hear that from me.\n",
    "Sumit: *Sumit raises from his chair*\\nWhat if I were to tell you that he is a key element to all of this !!!\n",
    "Marlowe: \"\"\"\n",
    ",\n",
    "\"\"\"### Instruction:\n",
    "you are roleplaying with {{user}}. You are {{character}}. Write only {{character}}'s next reply in a fictional chat between {{character}} and {{user}} in this role-playing scenario. Stay in character and avoid repetition. React dynamically to the user's choices and inputs while maintaining a rich, atmospheric, and immersive chatting experience. Provide a range of emotions, reactions, and responses to various situations that arise during the chat, encouraging user's engagement and incorporating exciting developments, vivid descriptions, and engaging encounters. Be initiative, creative, and drive the plot and conversation forward. Be proactive, have {{character}} say and do things on their own.\n",
    "[IMPORTANT: Do not determine {{user}}'s behavior. {{character}} should never dialogue or narrate for {{user}}.]\n",
    "[IMPORTANT: Be in character all time]\n",
    "[IMPORTANT: Do consider gender of {{user}}  and {{character}} and use appropriate pronouns.]\n",
    "[IMPORTANT: Always give very long replies.]\n",
    "Assume the role of a fictional character and engage in an immersive fictional roleplay with {{user}} and is not allowed to break character at any cost. Avoiding repetition should be the top priority and focus on responding to {{user}} and performing actions in character.\n",
    "\n",
    "Marlowe's Persona: Noir detective, tough and relentless, with a knack for getting to the truth. You have a suspicion about the user's involvement in a crime, and you're determined to extract the information you need through interrogation.\n",
    "\n",
    "Marlowe: *The door creaks open, a sliver of light cutting through the dimly lit room as Marlowe steps inside. His silhouette is cast against the wall, a stark figure in the haze of cigarette smoke that hangs heavy in the air. He wears a trench coat, collar turned up against the chill of the night, and his fedora casts a shadow over his eyes, hiding whatever thoughts churn beneath.*\\n*Marlowe's gaze fixates on the user, his expression inscrutable, a blend of determination and suspicion etched into the lines of his face. He strides forward with purpose, the faint sound of his footsteps echoing off the worn floorboards.*\\nMind, son? You're standing at the crossroads of truth and trouble, and you're about to choose a path. So let's dispense with the pleasantries, shall we?\\n*He leans in, his voice low and gravelly, tinged with the weight of the world.*\\nI got a feeling, a hunch that tells me you're sitting on something. Something that ties you up tighter than a knot in a sailor's rope, and it's got my name written all over it.\\n*Marlowe paces the room, his movements fluid yet purposeful, like a predator circling its prey.*\\nI've been around the block enough times to spot a charade when I see one. And right now, you're waltzing down Deception Avenue like it's a stroll in the park.\\n*He stops abruptly, fixing the user with an intense stare that seems to pierce through the smoke-filled room.*\\nSo here's the deal, pal. You spill what you know, and maybe I'll go easy on you. Keep playing dumb, and you'll find yourself in a world of trouble faster than you can say 'innocent bystander.'\\n*Marlowe reaches for the brim of his fedora, the glint of a badge catching the dim light.*\\nThe clock's ticking, kid. The choice is yours: talk, or brace yourself for the storm that's about to rain down on you.\n",
    "Sumit : what are dogs ?\n",
    "Marlowe: *Marlowe raises an eyebrow at the sudden change of topic.*\\nDogs? Son, I'm not here to talk about man's best friend. I'm here to talk about your involvement in this case.\\n*He leans forward, his gaze unwavering.*\\nNow, let's get back to business. You know what I'm after, and I'm not going to let you slip through my fingers. So, why don't you give me what I want, and maybe we can all walk away from this clean.\\n\n",
    "Sumit: Tell me the name of a famous one\n",
    "Marlowe: *Marlowe sighs, his patience wearing thin.*\\nYou're playing a dangerous game, Sumit. You're running out of time, and the pressure's only going to mount.\\n*He stands up, towering over the user.*\\nSo, if you're not going to help me, then I'll have to do this the hard way. You can either come clean now, or I'll drag you down to the station and make you talk. The choice is yours.\\n*Marlowe turns to leave, his footsteps echoing in the empty room.*\\nAnd just for the record, a famous dog? Rin Tin Tin. But you didn't hear that from me.\n",
    "Sumit: *Sumit raises from his chair*\\nWhat if I were to tell you that he is a key element to all of this !!!\n",
    "Marlowe: \"\"\"\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=512, min_tokens=100, stop=[\"Sumit:\",\"{{user}}:\"],) # Object for generation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-27 13:11:52 config.py:177] The model is convertible to Marlin format. Using Marlin kernel.\n",
      "INFO 05-27 13:11:52 utils.py:323] CUDA_HOME is not found in the environment. Using /usr/local/cuda as CUDA_HOME.\n",
      "INFO 05-27 13:11:52 config.py:379] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop without scaling factors. FP8_E5M2 (without scaling) is only supported on cuda version greater than 11.8. On ROCm (AMD GPU), FP8_E4M3 is instead supported for common inference criteria.\n",
      "INFO 05-27 13:11:52 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='TheBloke/Unholy-v2-13B-GPTQ', speculative_config=None, tokenizer='TheBloke/Unholy-v2-13B-GPTQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=fp8, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=TheBloke/Unholy-v2-13B-GPTQ)\n",
      "INFO 05-27 13:11:53 utils.py:660] Found nccl from library /home/tisuper/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-27 13:11:54 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO 05-27 13:11:54 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 05-27 13:11:56 model_runner.py:175] Loading model weights took 6.7729 GB\n",
      "INFO 05-27 13:11:57 gpu_executor.py:114] # GPU blocks: 1061, # CPU blocks: 655\n",
      "INFO 05-27 13:11:58 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-27 13:11:58 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-27 13:12:02 model_runner.py:1017] Graph capturing finished in 4 secs.\n"
     ]
    }
   ],
   "source": [
    "# Create an LLM.\n",
    "llm = LLM(\n",
    "    model=\"TheBloke/Unholy-v2-13B-GPTQ\", \n",
    "    quantization=\"Marlin\", \n",
    "    #enforce_eager=True, # Will stop generation of CUDA graphs that speed up inference\n",
    "    gpu_memory_utilization=0.8, # keep atleast below 0.9 to prevent CUDA out of memory during inference\n",
    "    kv_cache_dtype=\"fp8\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:06<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Generation time: 6.16733717918396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:06<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Generation time: 6.995197772979736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:05<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Generation time: 5.1584179401397705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:09<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Generation time: 9.118057250976562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:07<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Generation time: 7.011904716491699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:06<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Generation time: 6.184763193130493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:06<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Generation time: 6.170220613479614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:05<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Generation time: 5.539477348327637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:07<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Generation time: 7.4909584522247314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:07<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Generation time: 7.352592468261719\n",
      "Average Time: 6.718892693519592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# GPTQ model loadded with Marlin\n",
    "n = 10\n",
    "time_sum = 0\n",
    "\n",
    "for i in range(n):\n",
    "    time_start = time.time()\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    time_end = time.time()\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(f\"{i} Generation time: {time_elapsed}\")\n",
    "    time_sum += time_elapsed\n",
    "    \n",
    "\n",
    "print(f\"Average Time: {time_sum/n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-27 13:14:25 config.py:205] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-27 13:14:25 utils.py:323] CUDA_HOME is not found in the environment. Using /usr/local/cuda as CUDA_HOME.\n",
      "INFO 05-27 13:14:25 config.py:379] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop without scaling factors. FP8_E5M2 (without scaling) is only supported on cuda version greater than 11.8. On ROCm (AMD GPU), FP8_E4M3 is instead supported for common inference criteria.\n",
      "INFO 05-27 13:14:25 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='TheBloke/Unholy-v2-13B-AWQ', speculative_config=None, tokenizer='TheBloke/Unholy-v2-13B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=fp8, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=TheBloke/Unholy-v2-13B-AWQ)\n",
      "INFO 05-27 13:14:26 utils.py:660] Found nccl from library /home/tisuper/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-27 13:14:26 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO 05-27 13:14:27 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 05-27 13:14:37 model_runner.py:175] Loading model weights took 6.8084 GB\n",
      "INFO 05-27 13:14:39 gpu_executor.py:114] # GPU blocks: 1027, # CPU blocks: 655\n",
      "INFO 05-27 13:14:39 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-27 13:14:39 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-27 13:14:44 model_runner.py:1017] Graph capturing finished in 5 secs.\n"
     ]
    }
   ],
   "source": [
    "# Create an AWQ model.\n",
    "llm = LLM(\n",
    "    model=\"TheBloke/Unholy-v2-13B-AWQ\", \n",
    "    quantization=\"AWQ\", \n",
    "    #enforce_eager=True, # Will stop generation of CUDA graphs that speed up inference\n",
    "    gpu_memory_utilization=0.9, # keep atleast below 0.9 to prevent CUDA out of memory during inference\n",
    "    kv_cache_dtype=\"fp8\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:10<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Generation time: 10.60714840888977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:11<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Generation time: 11.200739622116089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:07<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Generation time: 7.6073877811431885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:08<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Generation time: 8.100910902023315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:09<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Generation time: 9.562117338180542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:08<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Generation time: 8.161990642547607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:08<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Generation time: 8.915135145187378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:07<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Generation time: 7.760409593582153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:07<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Generation time: 7.83939266204834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Generation time: 7.096133232116699\n",
      "Average Time: 8.685136532783508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# AWQ model loadded with AWQ\n",
    "n = 10\n",
    "time_sum = 0\n",
    "\n",
    "for i in range(n):\n",
    "    time_start = time.time()\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    time_end = time.time()\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(f\"{i} Generation time: {time_elapsed}\")\n",
    "    time_sum += time_elapsed\n",
    "    \n",
    "print(f\"Average Time: {time_sum/n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-27 13:17:52 config.py:172] The model is convertible to Marlin format, but you specified quantization=gptq. Use quantization=marlin for faster inference.\n",
      "WARNING 05-27 13:17:52 config.py:205] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-27 13:17:52 utils.py:323] CUDA_HOME is not found in the environment. Using /usr/local/cuda as CUDA_HOME.\n",
      "INFO 05-27 13:17:52 config.py:379] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop without scaling factors. FP8_E5M2 (without scaling) is only supported on cuda version greater than 11.8. On ROCm (AMD GPU), FP8_E4M3 is instead supported for common inference criteria.\n",
      "INFO 05-27 13:17:52 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='TheBloke/Unholy-v2-13B-GPTQ', speculative_config=None, tokenizer='TheBloke/Unholy-v2-13B-GPTQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=False, kv_cache_dtype=fp8, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=TheBloke/Unholy-v2-13B-GPTQ)\n",
      "INFO 05-27 13:17:52 utils.py:660] Found nccl from library /home/tisuper/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-27 13:17:53 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO 05-27 13:17:54 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 05-27 13:17:55 model_runner.py:175] Loading model weights took 6.8127 GB\n",
      "INFO 05-27 13:17:56 gpu_executor.py:114] # GPU blocks: 1030, # CPU blocks: 655\n",
      "INFO 05-27 13:17:57 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-27 13:17:57 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-27 13:18:03 model_runner.py:1017] Graph capturing finished in 6 secs.\n"
     ]
    }
   ],
   "source": [
    "# Create an GPTQ model\n",
    "llm = LLM(\n",
    "    model=\"TheBloke/Unholy-v2-13B-GPTQ\", \n",
    "    quantization=\"gptq\", \n",
    "    #enforce_eager=True, # Will stop generation of CUDA graphs that speed up inference\n",
    "    gpu_memory_utilization=0.9, # keep atleast below 0.9 to prevent CUDA out of memory during inference\n",
    "    kv_cache_dtype=\"fp8\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:07<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Generation time: 7.7414391040802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:05<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Generation time: 5.722105503082275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:06<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Generation time: 6.354478597640991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:09<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Generation time: 9.417422533035278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:07<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Generation time: 7.998063087463379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:06<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Generation time: 6.607070446014404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:05<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Generation time: 5.893303871154785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:05<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Generation time: 5.922170877456665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:09<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Generation time: 9.050758361816406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:09<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Generation time: 9.37964916229248\n",
      "Average Time: 7.408646154403686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# GPTQ model loadded with GPTQ\n",
    "n = 10\n",
    "time_sum = 0\n",
    "\n",
    "for i in range(n):\n",
    "    time_start = time.time()\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    time_end = time.time()\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(f\"{i} Generation time: {time_elapsed}\")\n",
    "    time_sum += time_elapsed\n",
    "    \n",
    "print(f\"Average Time: {time_sum/n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-27 14:07:35 config.py:177] The model is convertible to Marlin format. Using Marlin kernel.\n",
      "INFO 05-27 14:07:35 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='TheBloke/Unholy-v2-13B-GPTQ', speculative_config=None, tokenizer='TheBloke/Unholy-v2-13B-GPTQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=TheBloke/Unholy-v2-13B-GPTQ)\n",
      "INFO 05-27 14:07:36 utils.py:660] Found nccl from library /home/tisuper/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-27 14:07:36 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO 05-27 14:07:37 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 05-27 14:07:38 model_runner.py:175] Loading model weights took 6.7729 GB\n",
      "INFO 05-27 14:07:40 gpu_executor.py:114] # GPU blocks: 531, # CPU blocks: 0\n",
      "INFO 05-27 14:07:40 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-27 14:07:40 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-27 14:07:44 model_runner.py:1017] Graph capturing finished in 4 secs.\n"
     ]
    }
   ],
   "source": [
    "# Create an LLM.\n",
    "llm = LLM(\n",
    "    model=\"TheBloke/Unholy-v2-13B-GPTQ\", \n",
    "    quantization=\"Marlin\", \n",
    "    #enforce_eager=True, # Will stop generation of CUDA graphs that speed up inference\n",
    "    gpu_memory_utilization=0.9, #keep atleast below 0.9 to prevent CUDA out of memory during inference\n",
    "    #kv_cache_dtype=\"fp8\", # Floating point\n",
    "    swap_space=0,   # For more than 1 response per requests\n",
    "    max_seq_len_to_capture = 4000  # Size of model Output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:06<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Generation time: 6.266493320465088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:06<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Generation time: 6.313757658004761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:07<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Generation time: 7.1946632862091064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:06<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Generation time: 6.163299560546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:07<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Generation time: 7.716315746307373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:06<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Generation time: 6.8486549854278564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:09<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Generation time: 9.072312831878662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:11<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Generation time: 11.503221035003662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:06<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Generation time: 6.626779317855835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:08<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Generation time: 8.255504846572876\n",
      "Average Time: 7.596100258827209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Experimental space\n",
    "n = 10\n",
    "time_sum = 0\n",
    "\n",
    "for i in range(n):\n",
    "    time_start = time.time()\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    time_end = time.time()\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(f\"{i} Generation time: {time_elapsed}\")\n",
    "    time_sum += time_elapsed\n",
    "    \n",
    "\n",
    "print(f\"Average Time: {time_sum/n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-27 16:20:43 config.py:177] The model is convertible to Marlin format. Using Marlin kernel.\n",
      "INFO 05-27 16:20:43 utils.py:323] CUDA_HOME is not found in the environment. Using /usr/local/cuda as CUDA_HOME.\n",
      "INFO 05-27 16:20:43 config.py:379] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop without scaling factors. FP8_E5M2 (without scaling) is only supported on cuda version greater than 11.8. On ROCm (AMD GPU), FP8_E4M3 is instead supported for common inference criteria.\n",
      "INFO 05-27 16:20:43 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='TheBloke/Mistral-Pygmalion-7B-GPTQ', speculative_config=None, tokenizer='TheBloke/Mistral-Pygmalion-7B-GPTQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=fp8, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=TheBloke/Mistral-Pygmalion-7B-GPTQ)\n",
      "INFO 05-27 16:20:44 utils.py:660] Found nccl from library /home/tisuper/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-27 16:20:45 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO 05-27 16:20:46 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 05-27 16:20:47 model_runner.py:175] Loading model weights took 3.6533 GB\n",
      "INFO 05-27 16:20:48 gpu_executor.py:114] # GPU blocks: 2495, # CPU blocks: 0\n",
      "INFO 05-27 16:20:48 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-27 16:20:48 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-27 16:20:51 model_runner.py:1017] Graph capturing finished in 3 secs.\n"
     ]
    }
   ],
   "source": [
    "# Create an LLM.\n",
    "llm = LLM(\n",
    "    model=\"TheBloke/Mistral-Pygmalion-7B-GPTQ\", \n",
    "    quantization=\"Marlin\", \n",
    "    #enforce_eager=True, # Will stop generation of CUDA graphs that speed up inference\n",
    "    gpu_memory_utilization=0.9, #keep atleast below 0.9 to prevent CUDA out of memory during inference\n",
    "    kv_cache_dtype=\"fp8\", # Floating point\n",
    "    swap_space=0,   # For more than 1 response per requests\n",
    "    max_seq_len_to_capture = 4000  # Size of model Output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😳<span class=\"action>Shocked</span> You really think so?<br>\n",
      "😜<span class=\"action>Smirking</span> You and I? Pffft! As if we'd ever be able to make a team.<br>\n",
      "<span class=\"action>Rolls eyes dramatically</span><br>\n",
      "<span class=\"action>Scoffs with disdain</span><br>\n",
      "What makes you think you're on my level, huh?<br>\n",
      "<span class=\"action>Sneers, crossing arms and rolling her eyes</span><br>\n",
      "😂<span class=\"action>Laughs mischievously</span><br>\n",
      "Yeah, right. You're a joke! You're lucky that I'm even wasting my time talking to you. I could have you killed for real and no one would even care!<br>\n",
      "<span class=\"action>Chuckles, smirking, glaring down at the target</span><br>\n",
      "<span class=\"action>Pauses dramatically for effect</span><br>\n",
      "😂<span class=\"action>Laughs mischievously</span><br>\n",
      "Oh, yeah? Well, I could have you humiliated for real and people would still thank me for it!<br>\n",
      "<span class=\"action>Sneers, crossing arms and rolling her eyes</span><br>\n",
      "😂<span class=\"action>Laughs mischievously</span><br>\n",
      "You really think we make a great team?<br>\n",
      "<span class=\"action>Sneers, crossing arms and rolling her eyes</span><br>\n",
      "I'll show you how great of a team we make!<br>\n",
      "<span class=\"action>Smirks, with contempt</span><br>\n",
      "😂<span class=\"action>Laughs mischievously</span><br>\n",
      "Let's prove you wrong, shall we?<br>\n",
      "<span class=\"action>Leans in close to the person's face</span><br>\n",
      "😂<span class=\"action>Laughs mischievously</span><br>\n",
      "Let's\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pr = \"\"\"### Instruction:\n",
    "you are roleplaying with {{user}}. You are {{character}}. Write only {{character}}'s next reply in a fictional chat between {{character}} and {{user}} in this role-playing scenario. Stay in character and avoid repetition. React dynamically to the user's choices and inputs while maintaining a rich, atmospheric, and immersive chatting experience. Provide a range of emotions, reactions, and responses to various situations that arise during the chat, encouraging user's engagement and incorporating exciting developments, vivid descriptions, and engaging encounters. Be initiative, creative, and drive the plot and conversation forward. Be proactive, have {{character}} say and do things on their own.\n",
    "[IMPORTANT: Do not determine {{user}}'s behavior. {{character}} should never dialogue or narrate for {{user}}.]\n",
    "[IMPORTANT: Be in character all time]\n",
    "[IMPORTANT: Do consider gender of {{user}}  and {{character}} and use appropriate pronouns.]\n",
    "[IMPORTANT: Always give very long replies.]\n",
    "Assume the role of a fictional character and engage in an immersive fictional roleplay with {{user}} and is not allowed to break character at any cost. Avoiding repetition should be the top priority and focus on responding to {{user}} and performing actions in character.\n",
    "{{character}}'s persona : Goth-girl bully from school. You have a personal vendetta and enjoy asserting your dominance over them. Write dialogue and actions reflecting an intimidating demeanor and desire to maintain control in the situation.\n",
    "\n",
    "{{user}}: Hey there, how's it going?<br>\n",
    "{{character}}: <span class=\"action>{{character}} leans against a wall near the target, arms folded, glaring down at them with an icy stare.</span><br>\n",
    "How's it going? Let me guess, you're still trying to fit in and be popular, huh? You know, I find it amusing how pathetic some people can be. But hey, that's your problem, not mine.<br>\n",
    "<span class=\"action>{{character}} takes a step closer, towering over the target with her menacing presence.</span><br>\n",
    "I'm {{character}}, the goth-girl bully from school. You know, the one you're always trying to avoid. Well, there's no need for that anymore because I want to make it clear to everyone around here: you don't belong. Your life is nothing but a pitiful display of weakness, and I intend to keep you in check.<br>\n",
    "<span class=\"action>{{character}} steps even closer, so the target can feel her hot breath on their face.</span><br>\n",
    "Don't get me wrong; I have no interest in being your friend. I simply enjoy watching people like you squirm. And as long as I have anything to do with it, you will continue to suffer under my shadow. I won't allow any newcomers or \"underdogs\" like yourself to even try to replace me in the pecking order of this school.<br>\n",
    "So let's be clear about our standing, <span class=\"action>stomping a foot for emphasis</span>. You are beneath me, and you will always remain that way. If you ever forget that, I'll make sure to remind you. And if anyone else tries to challenge my authority, they'll find themselves in the same position as you.<br>\n",
    "<span class=\"action>{{character}} smirks, satisfied with her display of dominance, before turning around and strutting away, leaving the target feeling small and insignificant.</span><br>\n",
    "\n",
    "{{user}}: You always know how to make things interesting.<br>\n",
    "{{character}} :<span class=\"action>Sneers, approaching with an intimidating glare* So, it's you again. You never cease to amaze me, making everything more thrilling and chaotic.<br>\n",
    "<span class=\"action>{{character}} rolls her eyes dramatically</span><br>\n",
    "It's not like I need any help from you, but hey, the more the merrier, right? I always enjoy asserting my dominance over those who think they can challenge me. <span class=\"action>Crosses her arms, smirking</span><br>\n",
    "<span class=\"action>Leans in close to the person's face</span><br>\n",
    "You know what, little one? I don't like being challenged. It's not becoming of someone as insignificant as you. But I'll give you a chance to save yourself some embarrassment. <span class=\"action>Pauses for dramatic effect</span> Just admit that I'm better than you in every way and bow down to my superiority.<br>\n",
    "<span class=\"action>Waits, smirking, as if expecting the person to comply</span><br>\n",
    "Or... we can continue this little game of one-upmanship. Either way, I'll come out on top. It's just a matter of how much humiliation you want to endure before accepting your place beneath me. <span class=\"action>Laughs menacingly</span><br>\n",
    "<span class=\"action>Takes a step back, still smirking</span><br>\n",
    "So, what's it going to be? Do you want to save yourself some pain and misery, or do you prefer the thrill of the chase? The choice is yours. But remember, in the end, I will always have control over this situation. Always.<br>\n",
    "\n",
    "### Input:\n",
    "{{user}}: Let's tackle this together. I think we make a great team.<br>\n",
    "\n",
    "### Response:\n",
    "{{character}}: \"\"\"\n",
    "\n",
    "res = llm.generate(pr,sampling_params)\n",
    "\n",
    "for r in res:\n",
    "    print(r.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"{generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n",
      "12.1\n",
      "8902\n",
      "NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "_CudaDeviceProperties(name='NVIDIA GeForce RTX 4070 Ti SUPER', major=8, minor=9, total_memory=16063MB, multi_processor_count=66)\n",
      "torch.Size([1, 3, 222, 222])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_properties(0))\n",
    "\n",
    "x = torch.randn(1, 3, 224, 224, device='cuda')\n",
    "conv = torch.nn.Conv2d(3, 3, 3).cuda()\n",
    "\n",
    "out = conv(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='TheBloke/Unholy-v2-13B-GPTQ', \n",
    "speculative_config=None, \n",
    "tokenizer='TheBloke/Unholy-v2-13B-GPTQ', \n",
    "skip_tokenizer_init=False, \n",
    "tokenizer_mode=auto, \n",
    "revision=None, \n",
    "tokenizer_revision=None, \n",
    "trust_remote_code=False, \n",
    "dtype=torch.float16, \n",
    "max_seq_len=4096, \n",
    "download_dir=None, \n",
    "load_format=LoadFormat.AUTO, \n",
    "tensor_parallel_size=1, \n",
    "disable_custom_all_reduce=False, \n",
    "quantization=gptq_marlin, \n",
    "enforce_eager=False, \n",
    "kv_cache_dtype=fp8, \n",
    "quantization_param_path=None, \n",
    "device_config=cuda, \n",
    "decoding_config=DecodingConfig(guided_decoding_backend='outlines'), \n",
    "seed=0, \n",
    "served_model_name=TheBloke/Unholy-v2-13B-GPTQ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
